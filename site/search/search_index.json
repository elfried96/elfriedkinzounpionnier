{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Accueil","text":""},{"location":"#projet-classification-des-maladies-du-manioc","title":"Projet : Classification des maladies du manioc","text":""},{"location":"#presentation-generale","title":"Pr\u00e9sentation g\u00e9n\u00e9rale","text":"<p>Ce projet vise \u00e0 d\u00e9velopper un pipeline complet de classification multiclasse pour identifier automatiquement les maladies du manioc \u00e0 partir d\u2019images : - Passage d\u2019une approche d\u2019annotation via Azure AI \u00e0 un mod\u00e8le PyTorch entra\u00een\u00e9 sur vos images. - Structure reproduisible, tests, notebooks d\u2019exploration et site de documentation auto-d\u00e9ploy\u00e9.</p>"},{"location":"#contexte","title":"Contexte","text":"<p>Le manioc est une culture critique en Afrique de l\u2019Ouest, soumise \u00e0 des maladies comme : - Cassava Mosaic Disease (CMD) - Cassava Brown Streak Disease (CBSD) - Cassava Bacterial Blight (CBB) - Cassava Green Mottle - Plantes saines </p> <p>L\u2019automatisation de la d\u00e9tection et de la classification acc\u00e9l\u00e8re la surveillance des cultures et la cr\u00e9ation de datasets annot\u00e9s de haute qualit\u00e9.</p>"},{"location":"#objectifs","title":"Objectifs","text":"<ul> <li>Principal : Classifier 5 \u00e9tats/maladies du manioc via un mod\u00e8le CNN PyTorch.  </li> <li>Sp\u00e9cifiques :  </li> <li>Pr\u00e9traiter et organiser le dataset (raw \u2192 processed).  </li> <li>G\u00e9rer l\u2019\u00e9quilibre des classes (pond\u00e9ration de la perte &amp; sampler pond\u00e9r\u00e9).  </li> <li>Impl\u00e9menter et comparer un CNN personnalis\u00e9 et un mod\u00e8le pr\u00e9-entra\u00een\u00e9.  </li> <li>Entra\u00eener, \u00e9valuer (courbes, matrice de confusion) et explicabiliser (Grad-CAM).  </li> <li>Documenter chaque \u00e9tape et d\u00e9ployer via MkDocs + GitHub Pages.</li> </ul>"},{"location":"#technologies","title":"Technologies","text":"Outil / Langage Usage principal Python &amp; PyTorch Mod\u00e9lisation, entra\u00eenement, \u00e9valuation torchvision <code>ImageFolder</code>, transforms, mod\u00e8les pr\u00e9-entra\u00een\u00e9s Matplotlib &amp; Seaborn Visualisation (courbes, matrices de confusion) Jupyter Notebooks Exploration, d\u00e9mo interactive MkDocs + Material G\u00e9n\u00e9ration du site de documentation GitHub Actions CI pour tests et d\u00e9ploiement documentaire"},{"location":"#structure-du-depot","title":"Structure du d\u00e9p\u00f4t","text":"<pre><code>elfriedkinzounpionniers/\n\u251c\u2500\u2500 ci/                    # Environnements conda, CI\n\u251c\u2500\u2500 data/                  # Donn\u00e9es brutes et processing\n\u2502   \u251c\u2500\u2500 raw/               # Images par classe\n\u2502   \u251c\u2500\u2500 processed/         # train/val/test organis\u00e9s\n\u2502   \u2514\u2500\u2500 metadata.csv       # M\u00e9tadonn\u00e9es du dataset\n\u251c\u2500\u2500 docs/                  # Documentation MkDocs\n\u2502   \u251c\u2500\u2500 index.md           # Cette page\n\u2502   \u251c\u2500\u2500 data.md            # Dashboard dataset &amp; gallery\n\u2502   \u251c\u2500\u2500 pipeline.md        # Pipeline d\u00e9taill\u00e9 (pr\u00e9traitement \u2192 \u00e9valuation)\n\u2502   \u251c\u2500\u2500 model.md           # Architectures et choix de mod\u00e8le\n\u2502   \u251c\u2500\u2500 training.md        # Entra\u00eenement &amp; hyperparam\u00e8tres\n\u2502   \u2514\u2500\u2500 evaluation.md      # R\u00e9sultats, matrices, explicabilit\u00e9\n\u251c\u2500\u2500 models/                # D\u00e9finitions des architectures PyTorch\n\u2502   \u251c\u2500\u2500 classifier.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 notebooks/             # Notebooks d\u2019exploration &amp; d\u00e9mo\n\u251c\u2500\u2500 scripts/               # Outils (split, preprocess, inference)\n\u251c\u2500\u2500 training.py            # Fonctions train/predict/save/load\n\u251c\u2500\u2500 environment.yml        # D\u00e9pendances\n\u251c\u2500\u2500 README.md              # Lien vers docs/index.md\n\u2514\u2500\u2500 tests/                 # Tests unitaires\n</code></pre>"},{"location":"#licence","title":"\ud83d\udcc4 Licence","text":"<p>Distribu\u00e9 sous licence MIT. Voir le fichier LICENSE pour plus de d\u00e9tails.</p>"},{"location":"#contact","title":"\ud83d\udcde Contact","text":"<p>Elfried Kinzoun \u2022 ekfriedkinzoun@gmail.com GitHub : elfried96</p>"},{"location":"apk/","title":"Cassava Ol\u00f9\u1e63\u1ecd\u0301 - Diagnostic Intelligent des Maladies du Manioc","text":""},{"location":"apk/#a-propos-du-projet","title":"\ud83c\udf3f \u00c0 Propos du Projet","text":"<p>Cassava Ol\u00f9\u1e63\u1ecd\u0301 est une application web innovante d\u00e9di\u00e9e \u00e0 la classification automatique et \u00e0 la description des maladies du manioc \u00e0 l'aide de l'intelligence artificielle.</p>"},{"location":"apk/#objectif-principal","title":"\ud83c\udfaf Objectif Principal","text":"<p>Aider les agriculteurs et les chercheurs \u00e0 identifier rapidement et pr\u00e9cis\u00e9ment les maladies affectant les plants de manioc, en utilisant des technologies d'intelligence artificielle avanc\u00e9es.</p>"},{"location":"apk/#technologies-utilisees","title":"\ud83d\udee0 Technologies Utilis\u00e9es","text":""},{"location":"apk/#frontend","title":"Frontend","text":"<ul> <li>React 18</li> <li>TypeScript</li> <li>Vite</li> <li>Tailwind CSS</li> <li>shadcn/ui</li> <li>React Query</li> <li>React Router</li> <li>Framer Motion</li> </ul>"},{"location":"apk/#backend-futur-developpement","title":"Backend (Futur D\u00e9veloppement)","text":"<ul> <li>FastAPI</li> <li>SQLAlchemy</li> <li>Pydantic</li> <li>JWT Authentication</li> <li>SQLite (d\u00e9veloppement)</li> <li>PostgreSQL (production)</li> </ul>"},{"location":"apk/#fonctionnalites","title":"\ud83d\ude80 Fonctionnalit\u00e9s","text":"<ul> <li>\ud83d\udcf8 T\u00e9l\u00e9chargement d'images de feuilles de manioc</li> <li>\ud83d\udd0d Classification automatique des maladies</li> <li>\ud83d\udcdd G\u00e9n\u00e9ration de descriptions d\u00e9taill\u00e9es</li> <li>\ud83d\udc65 Gestion des utilisateurs et des droits</li> </ul>"},{"location":"apk/#prerequis","title":"\ud83d\udce6 Pr\u00e9requis","text":"<ul> <li>Node.js (v18+)</li> <li>npm ou yarn</li> <li>Python 3.9+ (pour le backend)</li> </ul>"},{"location":"apk/#installation","title":"\ud83d\udd27 Installation","text":""},{"location":"apk/#frontend_1","title":"Frontend","text":"<pre><code># Cloner le repository\ngit clone https://github.com/elfried96/cassava-vision-guard.git\n\n# Naviguer dans le dossier du projet\ncd cassava-olu\u1e63\u1ecd\u0301\n\n# Installer les d\u00e9pendances\nnpm install\n\n# Lancer le serveur de d\u00e9veloppement\nnpm run dev\n</code></pre>"},{"location":"apk/#backend-todo","title":"Backend (TODO)","text":"<pre><code># Installation des d\u00e9pendances Python\npip install fastapi uvicorn sqlalchemy pydantic python-jose[cryptography]\n\n# Lancer le serveur Backend\nuvicorn main:app --reload\n</code></pre>"},{"location":"apk/#structure-du-projet","title":"\ud83d\uddc2 Structure du Projet","text":"<pre><code>cassava-olu\u1e63\u1ecd\u0301/\n\u2502\n\u251c\u2500\u2500 frontend/\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 pages/\n\u2502   \u2502   \u251c\u2500\u2500 hooks/\n\u2502   \u2502   \u2514\u2500\u2500 utils/\n\u2502\n\u2514\u2500\u2500 backend/\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 models.py\n    \u251c\u2500\u2500 schemas.py\n    \u2514\u2500\u2500 ml_model/\n</code></pre>"},{"location":"apk/#licence","title":"\ud83d\udcc4 Licence","text":"<p>Distribu\u00e9 sous licence MIT.</p>"},{"location":"apk/#contact","title":"\ud83d\udcde Contact","text":"<p>Votre Nom - ekfriedkinzoun@gmail.com</p> <p>Projet Link: https://github.com/elfried96/cassava-vision-guard.git</p>"},{"location":"conclusion/","title":"\u2705 Conclusion et perspectives","text":"<p>Ce document r\u00e9sume les r\u00e9alisations du projet de classification des maladies du manioc et propose des axes d'am\u00e9lioration pour les travaux futurs.</p>"},{"location":"conclusion/#resume-des-realisations","title":"\ud83c\udfaf R\u00e9sum\u00e9 des r\u00e9alisations","text":"<ul> <li>Pipeline complet : Mise en place d'un pipeline de classification d'images, de la pr\u00e9paration des donn\u00e9es \u00e0 l'\u00e9valuation du mod\u00e8le.</li> <li>Gestion du d\u00e9s\u00e9quilibre : Application de techniques pour att\u00e9nuer l'impact du d\u00e9s\u00e9quilibre des classes.</li> <li>Mod\u00e9lisation efficace : Utilisation de CNN personnalis\u00e9s et de mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour am\u00e9liorer la performance.</li> <li>Explicabilit\u00e9 : Int\u00e9gration de m\u00e9thodes pour interpr\u00e9ter les d\u00e9cisions du mod\u00e8le.</li> </ul>"},{"location":"conclusion/#resultats-obtenus","title":"\ud83d\udcc8 R\u00e9sultats obtenus","text":"<ul> <li>Pr\u00e9cision globale : [Ins\u00e9rer la valeur obtenue].</li> <li>Classes les mieux reconnues : [Lister les classes avec les meilleures performances].</li> <li>Am\u00e9liorations observ\u00e9es : R\u00e9duction de l'erreur sur les classes minoritaires gr\u00e2ce \u00e0 la pond\u00e9ration de la perte.</li> </ul>"},{"location":"conclusion/#perspectives-damelioration","title":"\ud83d\ude80 Perspectives d'am\u00e9lioration","text":"<ul> <li>Augmentation des donn\u00e9es : Collecte de nouvelles images pour enrichir le dataset.</li> <li>Optimisation des hyperparam\u00e8tres : Exploration de diff\u00e9rentes configurations pour am\u00e9liorer la performance.</li> <li>D\u00e9ploiement : Am\u00e9lioration de l'interface utilisateur pour faciliter l'utilisation du mod\u00e8le.</li> </ul>"},{"location":"conclusion/#ressources-supplementaires","title":"\ud83d\udcda Ressources suppl\u00e9mentaires","text":"<ul> <li>Documentation PyTorch</li> <li>Tutoriels PyTorch</li> <li>TorchCAM</li> </ul>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#nous-contacter","title":"Nous contacter","text":"<p>Si vous avez des questions ou souhaitez entrer en contact avec nous, veuillez utiliser les informations ci-dessous :</p>"},{"location":"contact/#adresse","title":"Adresse","text":"<p>Abomey Calavi</p>"},{"location":"contact/#telephone","title":"T\u00e9l\u00e9phone","text":"<p>+229 0196848729</p>"},{"location":"contact/#email","title":"Email","text":"<p>ekfriedkinzoun@gmail.com</p>"},{"location":"contact/#reseaux-sociaux","title":"R\u00e9seaux sociaux","text":"<ul> <li> <p>Facebook</p> </li> <li> <p>LinkedIn</p> </li> </ul> <p>Nous sommes impatients de vous entendre !</p>"},{"location":"data/","title":"data","text":""},{"location":"data/#apercu-du-dataset","title":"\ud83e\uddec Aper\u00e7u du Dataset","text":"<p>Le dataset est constitu\u00e9 de X images de feuilles de manioc class\u00e9es en 8 cat\u00e9gories de sant\u00e9/maladies. Chaque image a \u00e9t\u00e9 annot\u00e9e manuellement et pr\u00e9trait\u00e9e pour entra\u00eener un mod\u00e8le de classification.</p> <ul> <li>\ud83d\udce6 Total d'images : X</li> <li>\ud83d\udcc2 Classes : 5</li> <li>\ud83d\uddbc\ufe0f Taille normalis\u00e9e : 224x224 pixels</li> </ul>"},{"location":"data/#repartition-des-classes-pour-le-train","title":"\ud83d\udcca R\u00e9partition des classes Pour le train","text":""},{"location":"data/#tableau-resume","title":"Tableau r\u00e9sum\u00e9","text":"Classe Nombre d'images Description courte Exemple Cassava Mosaic 13.2k Feuilles avec motifs marbr\u00e9s irr\u00e9guliers Cassava Bacterial Blight 1087 Feuilles perfor\u00e9es avec n\u00e9crose humide Cassava Healthy 2577 Feuilles saines, vertes homog\u00e8nes Cassava Brown Streak Disease 2189 Taches brunes le long des nervures Cassava Green Mottle 2338 Feuilles saines, vertes homog\u00e8nes <p>Pour plus d'informtion veiller consulter  Kaggle</p> <p>mkdocs serve</p>"},{"location":"installation/","title":"\ud83d\udce6 Guide d'Installation","text":"<p>Ce guide vous aide \u00e0 installer et \u00e0 lancer l\u2019application Cassava Ol\u00f9\u1e63\u1ecd\u0301 en local, en configurant \u00e0 la fois le frontend (interface utilisateur) et le backend (API + pipelines ML).</p>"},{"location":"installation/#prerequis","title":"\u2705 Pr\u00e9requis","text":"<p>Assurez-vous d\u2019avoir les outils suivants install\u00e9s :</p> <ul> <li>Node.js v18+ avec <code>npm</code> ou <code>yarn</code></li> <li>Python 3.9+ (recommand\u00e9 : via Miniconda)</li> <li>Conda pour g\u00e9rer l\u2019environnement Python</li> <li>Une cl\u00e9 API Azure OpenAI avec acc\u00e8s \u00e0 GPT-4 Vision (obligatoire pour la classification par vision)</li> <li>Git</li> </ul>"},{"location":"installation/#1-cloner-le-depot","title":"1\ufe0f\u20e3 Cloner le d\u00e9p\u00f4t","text":"<pre><code>git clone https://github.com/elfried96/cassava-vision-guard.git\ncd cassava-vision-guard\n</code></pre>"},{"location":"installation/#2-installation-du-frontend-react","title":"2\ufe0f\u20e3 Installation du Frontend (React)","text":"<pre><code>cd frontend\nnpm install         # ou yarn install\nnpm run dev         # ou yarn dev\n</code></pre> <p>Cela d\u00e9marre le serveur sur : \ud83d\udccd <code>http://localhost:3000</code></p>"},{"location":"installation/#3-configuration-de-lenvironnement-python","title":"3\ufe0f\u20e3 Configuration de l\u2019Environnement Python","text":"<p>Depuis la racine du projet :</p> <pre><code>conda create -n cassava python=3.9 --yes\nconda activate cassava\n</code></pre> <p>Ensuite installez les d\u00e9pendances :</p> <pre><code>pip install -r requirements.txt\n# OU utilisez le fichier conda\n# conda env create -f environment.yml\n</code></pre>"},{"location":"installation/#4-lancement-du-backend-fastapi","title":"4\ufe0f\u20e3 Lancement du Backend (FastAPI)","text":"<pre><code>pip install \"fastapi[standard]\" uvicorn[standard] sqlalchemy pydantic python-dotenv\nuvicorn backend.main:app --reload\n</code></pre> <p>L\u2019API sera disponible sur : \ud83d\udccd <code>http://localhost:8000</code></p>"},{"location":"installation/#5-variables-denvironnement","title":"5\ufe0f\u20e3 Variables d\u2019Environnement","text":"<p>Cr\u00e9ez un fichier <code>.env</code> \u00e0 la racine du projet :</p> <pre><code>AZURE_OPENAI_KEY=votre_cl\u00e9_openai\nFASTAPI_ENV=development\n</code></pre> <p>Ensuite, exportez les variables (sous Linux/macOS) :</p> <pre><code>export $(grep -v '^#' .env | xargs)\n</code></pre> <p>Sous Windows (CMD) :</p> <pre><code>for /f \"delims=\" %i in (.env) do set %i\n</code></pre>"},{"location":"installation/#resume-des-ports","title":"\u2705 R\u00e9sum\u00e9 des ports","text":"Composant Port par d\u00e9faut Frontend <code>http://localhost:3000</code> Backend <code>http://localhost:8000</code>"},{"location":"installation/#test-de-bon-fonctionnement","title":"\ud83e\uddea Test de bon fonctionnement","text":"<ul> <li>Lancez le frontend (<code>npm run dev</code>)</li> <li>Lancez l\u2019API (<code>uvicorn backend.main:app --reload</code>)</li> <li>Acc\u00e9dez \u00e0 <code>http://localhost:3000</code> et chargez une image de feuille de manioc pour tester</li> </ul>"},{"location":"methodologie/","title":"M\u00e9thodologie","text":""},{"location":"methodologie/#methodologie-du-pipeline-de-classification-et-dannotation","title":"M\u00e9thodologie du Pipeline de Classification et d\u2019Annotation","text":"<p>Ce document d\u00e9crit le pipeline actualis\u00e9, combinant \u00e0 la fois l\u2019annotation automatique des sc\u00e8nes et la classification multiclasse, ainsi que l\u2019int\u00e9gration de l\u2019application web Cassava Ol\u00f9\u1e63\u1ecd\u0301.</p>"},{"location":"methodologie/#1-collecte-et-description-des-scenes","title":"1. Collecte et Description des Sc\u00e8nes","text":"<ol> <li> <p>Acquisition des images</p> </li> <li> <p>Photos de feuilles de manioc prises in situ et issues de bases ouvertes.</p> </li> <li> <p>Organisation en cinq cat\u00e9gories :</p> <ul> <li>Cassava Bacterial Blight</li> <li>Cassava Brown Streak Disease</li> <li>Cassava Green Mottle</li> <li>Cassava Mosaic Disease</li> <li>Cassava Healthy</li> </ul> </li> <li> <p>Pr\u00e9traitement des images</p> </li> <li> <p>Redimensionnement \u00e0 224\u00d7224 px, conversion RGB\u00a0</p> </li> <li>Normalisation (moyennes et \u00e9carts\u2011types calcul\u00e9s sur le jeu complet)</li> <li>S\u00e9paration en train, validation, test (70/15/15) avec copie des fichiers dans <code>/processing</code></li> </ol>"},{"location":"methodologie/#2-pipeline-de-classification-multiclasse-pytorch","title":"2. Pipeline de Classification Multiclasse (PyTorch)","text":"<ol> <li> <p>Construction du mod\u00e8le</p> </li> <li> <p>CNN personnalis\u00e9 (<code>CassavaCNN</code>) ou mod\u00e8le pr\u00e9\u2011entra\u00een\u00e9 (ResNet18) fine\u2011tuned</p> </li> <li> <p>Trois blocs conv + pool, deux couches fully\u2011connected, sortie 5 classes</p> </li> <li> <p>Gestion du d\u00e9s\u00e9quilibre</p> </li> <li> <p>Calcul de poids inverses de fr\u00e9quence</p> </li> <li>Mise en place d\u2019un <code>WeightedRandomSampler</code> pour des batchs \u00e9quilibr\u00e9s</li> <li> <p>(Optionnel) Data augmentation cibl\u00e9e</p> </li> <li> <p>Boucle d\u2019entra\u00eenement</p> </li> <li> <p>Fonctions <code>train()</code> et <code>predict()</code> encapsul\u00e9es dans <code>training.py</code></p> </li> <li> <p>Enregistrement des m\u00e9triques de perte et de pr\u00e9cision par \u00e9poque</p> </li> <li> <p>\u00c9valuation et visualisation</p> </li> <li> <p>Courbes de loss / accuracy</p> </li> <li>Matrice de confusion via Scikit\u2011learn</li> <li>Saliency maps ou Grad\u2011CAM pour l\u2019explicabilit\u00e9</li> </ol> <ul> <li>Pr\u00e9dictions PyTorch </li> <li>Post\u2011traitement des scores (seuil, moyenne pond\u00e9r\u00e9e)</li> </ul>"},{"location":"methodologie/#3-annotation-automatique-via-gpt-4-vision","title":"3. Annotation Automatique via GPT-4 Vision","text":"<ol> <li> <p>Utilisation de l\u2019API GPT (Azure OpenAI)</p> </li> <li> <p>G\u00e9n\u00e9ration automatique de descriptions de sc\u00e8ne pour chaque image de feuille de manioc</p> </li> <li> <p>Analyse s\u00e9mantique de la sc\u00e8ne : niveau de fl\u00e9trissement, pr\u00e9sence de taches, teinte dominante, etc.</p> </li> <li> <p>Contribution \u00e0 l\u2019explicabilit\u00e9 du mod\u00e8le</p> </li> <li> <p>Les descriptions g\u00e9n\u00e9r\u00e9es sont utilis\u00e9es pour enrichir le retour utilisateur dans l\u2019application</p> </li> <li> <p>Elles permettent aussi de v\u00e9rifier la coh\u00e9rence entre la pr\u00e9diction du mod\u00e8le et l\u2019apparence r\u00e9elle de la feuille</p> </li> <li> <p>Post-traitement des r\u00e9sultats</p> </li> <li> <p>Alignement des descriptions avec les pr\u00e9dictions du mod\u00e8le PyTorch</p> </li> <li>Conservation des textes utiles dans la base pour annotation manuelle et audit de qualit\u00e9</li> </ol>"},{"location":"methodologie/#4-integration-a-lapplication-web","title":"4. Int\u00e9gration \u00e0 l\u2019Application Web","text":"<ol> <li> <p>Frontend Cassava Ol\u00f9\u1e63\u1ecd\u0301</p> </li> <li> <p>Composant d\u2019upload et aper\u00e7u imm\u00e9diat</p> </li> <li>Affichage du r\u00e9sultat (maladie + pourcentage de confiance)</li> <li> <p>Description des sympt\u00f4mes et recommandations li\u00e9es</p> </li> <li> <p>API Backend (FastAPI)</p> </li> <li> <p>Endpoint <code>/predict</code> acceptant une image et retournant JSON {classe, score, description}</p> </li> <li> <p>D\u00e9ploiement</p> </li> <li> <p>Dockerisation du service</p> </li> <li>CI/CD avec GitHub Actions pour tests et d\u00e9ploiement automatique</li> </ol> <p>Cette m\u00e9thodologie combin\u00e9e garantit une annotation pr\u00e9cise, une exp\u00e9rience utilisateur fluide et une reproductibilit\u00e9 acad\u00e9mique et industrielle.</p>"},{"location":"pipeline/01_preparation/","title":"\ud83d\udd04 Pipeline de classification des maladies du manioc","text":"<p>Ce document d\u00e9taille le pipeline complet mis en place pour la classification automatique des maladies du manioc \u00e0 partir d'images, en utilisant PyTorch.</p>"},{"location":"pipeline/01_preparation/#1-chargement-et-preparation-des-donnees","title":"1. \ud83d\udcc2 Chargement et pr\u00e9paration des donn\u00e9es","text":"<ul> <li>Source des donn\u00e9es : Images collect\u00e9es sur le terrain, organis\u00e9es par classe dans le r\u00e9pertoire <code>data/raw/</code>.</li> <li>Chargement : Utilisation de <code>torchvision.datasets.ImageFolder</code> pour structurer les donn\u00e9es.</li> <li>Transformations appliqu\u00e9es :</li> <li>Redimensionnement des images \u00e0 une taille uniforme (par exemple, 224x224 pixels).</li> <li>Normalisation des canaux RGB selon les moyennes et \u00e9carts-types d'ImageNet.</li> <li>Augmentations de donn\u00e9es (rotations, flips horizontaux) pour am\u00e9liorer la robustesse du mod\u00e8le.</li> </ul>"},{"location":"pipeline/01_preparation/#2-separation-du-jeu-de-donnees","title":"2. \ud83e\uddea S\u00e9paration du jeu de donn\u00e9es","text":"<ul> <li>R\u00e9partition :</li> <li>Entra\u00eenement : 70%</li> <li>Validation : 15%</li> <li>Test : 15%</li> <li>M\u00e9thode : Utilisation de <code>torch.utils.data.random_split</code> avec une graine fixe (<code>torch.Generator().manual_seed(42)</code>) pour assurer la reproductibilit\u00e9.</li> </ul>"},{"location":"pipeline/01_preparation/#3-gestion-du-desequilibre-des-classes","title":"3. \u2696\ufe0f Gestion du d\u00e9s\u00e9quilibre des classes","text":"<ul> <li>Analyse : Comptage des occurrences de chaque classe \u00e0 l'aide de <code>collections.Counter</code>.</li> <li>Techniques mises en \u0153uvre :</li> <li>Pond\u00e9ration de la fonction de perte : Calcul des poids inverses des fr\u00e9quences des classes pour <code>torch.nn.CrossEntropyLoss</code>.</li> <li>Sampler pond\u00e9r\u00e9 : Optionnellement, utilisation de <code>torch.utils.data.WeightedRandomSampler</code> pour \u00e9quilibrer les classes lors de l'entra\u00eenement.</li> </ul>"},{"location":"pipeline/01_preparation/#4-modelisation","title":"4. \ud83e\udde0 Mod\u00e9lisation","text":"<ul> <li>Architecture personnalis\u00e9e : Convolutional Neural Network (CNN) d\u00e9fini dans <code>models/classifier.py</code>.</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s : Int\u00e9gration de mod\u00e8les tels que <code>ResNet18</code> via <code>torchvision.models</code> pour le transfert d'apprentissage.</li> <li>Fonction de perte : <code>torch.nn.CrossEntropyLoss</code> avec ou sans pond\u00e9ration.</li> <li>Optimiseur : <code>torch.optim.Adam</code> avec un taux d'apprentissage initial de 1e-4.</li> </ul>"},{"location":"pipeline/01_preparation/#5-entrainement","title":"5. \ud83c\udfcb\ufe0f Entra\u00eenement","text":"<ul> <li>Boucle d'entra\u00eenement : Impl\u00e9ment\u00e9e dans <code>training.py</code>, avec suivi de la perte et de la pr\u00e9cision.</li> <li>Sauvegarde du mod\u00e8le : Enregistrement des poids du mod\u00e8le apr\u00e8s chaque \u00e9poque si la performance s'am\u00e9liore.</li> <li>Visualisation : Courbes de perte et de pr\u00e9cision g\u00e9n\u00e9r\u00e9es \u00e0 l'aide de <code>matplotlib</code>.</li> </ul>"},{"location":"pipeline/01_preparation/#6-evaluation","title":"6. \ud83d\udcca \u00c9valuation","text":"<ul> <li>M\u00e9triques :</li> <li>Pr\u00e9cision globale.</li> <li>Matrice de confusion.</li> <li>Rapport de classification (pr\u00e9cision, rappel, F1-score).</li> <li>Explicabilit\u00e9 : Utilisation de <code>torchcam</code> pour g\u00e9n\u00e9rer des cartes de chaleur (Grad-CAM) illustrant les zones d'attention du mod\u00e8le.</li> </ul>"},{"location":"pipeline/01_preparation/#7-organisation-des-fichiers","title":"7. \ud83d\udcc1 Organisation des fichiers","text":"<pre><code>data/\n\u251c\u2500\u2500 raw/                # Images originales\n\u251c\u2500\u2500 processed/          # Donn\u00e9es apr\u00e8s s\u00e9paration\nmodels/\n\u251c\u2500\u2500 classifier.py       # D\u00e9finition du mod\u00e8le\n\u251c\u2500\u2500 utils.py            # Fonctions utilitaires\nnotebooks/\n\u251c\u2500\u2500 exploration.ipynb   # Analyse exploratoire\n\u251c\u2500\u2500 training.ipynb      # Entra\u00eenement du mod\u00e8le\nscripts/\n\u251c\u2500\u2500 split_data.py       # Script de s\u00e9paration des donn\u00e9es\n\u251c\u2500\u2500 train_model.py      # Script d'entra\u00eenement\n</code></pre>"}]}